{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PURPOSE OF THE DATA WRANGLING MASTER TUTORIAL WITH R AND PYTHON\n",
    "The purpose of this Master Tutorial is to teach a bit of data wrangling on a typical social science data set derived from a survey. The purpose is NOT to teach you R or Python. That would be much better handled in a dedicated class on Coursera, Udemy, edX, etc (feel free to email me at robstilson@gmail.com if you would like recommendations). This tutorial jumps in at around an advanced beginner to intermediate level. Therefore the purpose is more around teaching data wrangling while laying the ground work for a Rosetta Stone type reference between R and Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will import the needed packages. This is similar to the library() command in R. Notice the \"as\" after the import of the package. This creates an alias so instead of typing pandas.read_csv to import a .csv we can just type pd.read_csv. Unlike R, in Python you are essentially calling the commands from the package everytime you ran them. This can help to alleviate the confusion of like named functions. For instance in R if you run the describe() function, it will run the describe() function from the last package you installed. For a social scientist, this can become confusing when you have both the Hmisc and psych package loaded simultaneaously. In R you can get around this by using ::, so psych::describe() and Hmisc::describe(). This is never an issue in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to import Jupyter Notebook (.ipynb) from GitHub\n",
    "From: https://stackoverflow.com/questions/45622602/how-to-jupyter-notebooks-from-github\n",
    "1. First click on Raw\n",
    "2. Then, press ctrl+s to save it as .ipynb\n",
    "3. Open jupyter notebook\n",
    "4. Go to location where you saved .ipynb file\n",
    "5. Open file, you will see the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv('C:\\\\Users\\\\Rob\\\\Desktop\\\\Python Programming Course\\\\SIOP Data Wrangling Master Tutorial Data Set.csv', encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to call the head of the data. This is essentially the same as head() in R. Notice however that instead of head(data) we are going to use data.head() with a period seperating the data frame from the function. This is why I tell those who I teach R to NEVER USE PERIODS in your naming conventions because if/when you also learn Python this can become very confusing.\n",
    "\n",
    "Also notice that we are grabbing the first 5 rows of the data, but they are indexed from 0:4. Python is a programming language that uses 0 indexing unlike R which uses 1 indexing. For more on this go [here](https://python-history.blogspot.com/2013/10/why-python-uses-0-based-indexing.html). Keep this in mind as it can also be very confusing when you are coming from an index 1 language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.info() is similar to str() in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.columns is similar to colnames() in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the string data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of [\\uXXXX] designations below. This is for unicode text and is particularly useful for some of the nastier text you may import and need to clean (em-dash, I'm looking at you). The site (What Unicode character is this?) [http://www.babelstone.co.uk/Unicode/whatisit.html?] is very helpful in helping to identify what characters you are trying to remove from your text. Some of the usual suspects are found below and you may want to build some code in at the beginning of your documents to simply handle this before it becomes a problem with variable or column names.\n",
    "Em-dash-u2014\n",
    "Curly quotes left-u201C\n",
    "Curly quotes right-u201D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions (regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also helpful to know about Regular Expressions (regex) when dealing with text data. If you are like me, you may be asking yourself, [\"What are Regular Expressions?\"](https://www.regular-expressions.info/) I had never heard of them until a few years after graduate school and as the linked site so eloquently puts it, regex is \"a special text string for describing a search pattern. You can think of regular expressions as wildcards on steroids.\" Regex is very powerful when trying to maniuplate text and I highly recommend learning more about it, especially if you are interested in Data Wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing \" \" in variable names and replacing with \"_\"\n",
    "#From: https://salayhin.wordpress.com/2016/07/09/rename-columns-in-pandas-data-frame/\n",
    "data.columns=data.columns.str.replace(' ', '_') #replacing spaces with underscores\n",
    "data.columns=data.columns.str.replace(r\"[\\\"\\',]\", '') \n",
    "data.columns=data.columns.str.replace(r\"[\\u2018\\u2019\\u201A\\u201B\\u2032\\u2035\\u0092\\u009d\\u0096\\u0093\\u0094\\u0099\\u0091]\", \"'\")\n",
    "data.columns=data.columns.str.replace(r\"[\\u201C\\u201D\\u201E\\u201F\\u2033\\u2036]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the columns to make sure the code worked.\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing columns\n",
    "Here we remove the Respondent_IP column because we don't need it. To remove a column in Python, make sure to include the inplace = True because that allows you to delete the column without having to reassign the df. See more about \"Why should I make a copy of a data frame in pandas\" by [following the link](https://stackoverflow.com/questions/27673231/why-should-i-make-a-copy-of-a-data-frame-in-pandas/39628860)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Respondent_IP\n",
    "#From: https://stackoverflow.com/questions/13411544/delete-column-from-pandas-dataframe\n",
    "data.drop('Respondent_IP', axis = 1, inplace=True) #axis = 0 for rows and 1 for columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to create new variable names for OEQs like below in R\n",
    "#colnames(Data)[56]=\"NPS_On a scale from zero to ten how likely is it that you would recommend Esquire to friends as a great place to work?\"\n",
    "#colnames(Data)[57]=\"OEQ_What 2-3 things do you value most about working at Esquire?\"\n",
    "#colnames(Data)[58]=\"OEQ_What 2-3 things should Esquire begin to do?\"\n",
    "#colnames(Data)[59]=\"OEQ_What 2-3 things should Esquire stop doing?\"\n",
    "#colnames(Data)[60]=\"OEQ_Please provide suggestions for ongoing improvement to the performance feedback process.\"\n",
    "\n",
    "#Also need to clean out quotes that are showing up as boxes above\n",
    "data.rename(columns={'On_a_scale_of_zero_to_ten_how_likely_is_it_that_you_would_recommend_Company_to_friends_as_a_great_place_to_work?': 'NPS:On a scale from zero to ten how likely is it that you would recommend Company to friends as a great place to work?',\\\n",
    "                     'What_2-3_things_do_you_value_most_about_working_at_Company?': 'OEQ:What_2-3_things_do_you_value_most_about_working_at_Company?',\\\n",
    "                     'What_2-3_things_should_Company_begin_to_do?':'OEQ:What_2-3_things_should_Company_begin_to_do?',\\\n",
    "                     'What_2-3_things_should_Company_stop_doing?':'OEQ:What_2-3_things_should_Company_stop_doing?',\\\n",
    "                     'Please_provide_suggestions_for_ongoing_improvement_to_the_performance_feedback_process.':'OEQ:Please_provide_suggestions_for_ongoing_improvement_to_the_performance_feedback_process.'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.columns #make sure to alwasy check your columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Data (e.g. Wide vs. tall, Melted vs. Casted, Gathered vs. Spread)\n",
    "[Tidy Data](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf) by Hadley Wickham is an excellent read on how your data frames should be set up. The way I try to explain it is that in I/O Psychology we typically work with 1 row per person. This leads to very wide data sets as more variables are added. Especially with multiple metrics over multiple years (12 metrics by 3 years of montly data becomes 432 columns of data). A tall data set on the other hand is multiple rows per person with fewer columns. This also allows me to write one line of code to get the mean, SD, etc. for all of those variables instead of individually coding each of them. It is probably better to explain this visually, so I will send you to the excellent tutorial by Sean C. Anderson [here](http://seananderson.ca/2013/10/19/reshape/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spaces in Python\n",
    "Since spaces matter in Python (unlike R), if you want to drop down a line, you must insert an escape, \"\\\" for your code to work properly. I prefer the below layout with a column of variables rather than a row of variables becuase it is easier for me to look at and if I need to make changes, it it easier for me to swap a variable in our out of the column without digging through rows of variable names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melting data in Python\n",
    "Melting (making tall or long) data in Python is fairly similar to how R does it with the reshape2 package. You will tell it the DF, in this case data, and then tell it the ID variables (e.g. not variables with values). Your new melted data frame will then contain those ID variables and will melt where all variables and their values become the last two columns. Don't fret if this seems confusing now. Start small and work your way up and I think once you start doing data manipulation with Tall data, it will become your go to method. It also makes data visualization much easier (in my opinion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_melt = pd.melt(data, id_vars=['Number', 'First_Name','Last_Name','Email_Address', 'Department','Division','Completed_On'])\n",
    "Data_melt = pd.melt(data, id_vars=['Number', \\\n",
    "                                   'First_Name', \\\n",
    "                                   'Last_Name', \\\n",
    "                                   'Email_Address', \\\n",
    "                                   'Department', \\\n",
    "                                   'Division', \\\n",
    "                                   'Completed_On'])\n",
    "#Make sure there are no spaces after your \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the head of the newly melted data. Notice the underscore for naming and the period for calling the function. I highly recommend you write your R code the same way but YMMV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt.columns #check the columns of the melted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting columns in Python\n",
    "Now with our data in tall form, we are going to split the variable column into Area (for broad survey topic area) and Item (for the specific item). In the code block below, I've included some false starts (now commented out) and the final method (uncommented) to show you that this is indeed a process. \n",
    "\n",
    "The method I ended up going with incorporates the Anonymous or [Lambda](http://www.secnetix.de/olli/Python/lambda_functions.hawk) function. We won't have time to get into this here, but just know that at a high level that a Lambda function is a function that is not bound to a name at runtime. If you got that, Great! If not, no worries, let's move on and you can circle back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting variable column on : to give us Area and Item\n",
    "#From: https://stackoverflow.com/questions/14745022/pandas-dataframe-how-do-i-split-a-column-into-two\n",
    "#Data_melt = pd.DataFrame(Data_melt.variable.str.split(':', 1, index = Data_melt.index), columns = ['Area', 'Item'])\n",
    "#Data_melt2 = pd.DataFrame(Data_melt.variable.str.split(':', 1).tolist(), columns = ['Area', 'Item'])#Try to append this to Data_melt \n",
    "#Data_melt = pd.DataFrame(Data_melt.variable.str.split(':', 1), columns = ['Area', 'Item'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Data_melt = Data_melt.join(s.apply(lambda x: Series(x.split(':'))))\n",
    "Data_melt['Area'], Data_melt['Item'] = zip(*Data_melt['variable'].apply(lambda x: x.split(':', 1)))\n",
    "#From:https://gist.github.com/bsweger/e5817488d161f37dcbd2 \n",
    "#Data_melt['Area', 'Item'] = zip(*Data_melt['variable'].apply(lambda x: x.split(':', 1)))\n",
    "#Data_melt['Area', 'Item'] = pd.DataFrame(Data_melt.variable.str.split(':', 1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt.columns #make sure to check your columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt.head() #and the head of the data just to put eyes on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing 'variable' since we no longer need it.\n",
    "#From: https://stackoverflow.com/questions/13411544/delete-column-from-pandas-dataframe\n",
    "Data_melt.drop('variable', axis = 1, inplace=True) #axis = 0 for rows and 1 for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt.head() #checking again to make sure it dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Aliases for data groups\n",
    "I like to create Aliases for data groups as it makes it easier for me to roll data up and serves as a proxy for those (including myself) who came from SPSS and got used to labels for long item names. Since neither Python or R do that, here is my work around. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding unique data in tall format\n",
    "We are going to find the unique (e.g. distinct) Item names in our melted data frame using df_melt.Item.unique(). Notice how we used our Data_melt plus Item plus unique() together with a period seperator. This is simmilar to \"piping\" with dplyr from the Tidyverse in R. \n",
    "\n",
    "One way I might use this is to then create a \"Helper\" file in Excel where I paste the results of the unique Items and then use concatenate with the necessary programming language. I will often do this if I need to write 20 or more similar lines of code for renaming, ifelse, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Aliases\n",
    "\n",
    "#Getting unique Item names\n",
    "#From: https://chrisalbon.com/python/pandas_list_unique_values_in_column.html\n",
    "Data_melt.Item.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will actually create the aliases in the data frame. There must be a better way to do this, but I haven't come across it yet. If you know of a better way, please let me know!\n",
    "\n",
    "So, we will create a new column in the df via Data_melt['Alias'] and assign it blanks (e.g. 'NA'). Then we will populate it with the .loc() function from the Pandas package. See more on this [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing \"_\" in Items with \" \" #Will come back to this\n",
    "#From: http://pandas.pydata.org/pandas-docs/version/0.17.0/text.html\n",
    "\n",
    "#pandas: Use if-else to populate new column\n",
    "#From: https://stackoverflow.com/questions/29153805/pandas-use-if-else-to-populate-new-column\n",
    "\n",
    "#Create \"TEST\" df\n",
    "#TEST = pd.DataFrame(Data_melt)\n",
    "\n",
    "#From: https://stackoverflow.com/questions/21733893/pandas-dataframe-add-a-field-based-on-multiple-if-statements\n",
    "Data_melt['Alias']='NA'\n",
    "#Data_melt.loc[Data_melt['Item'] == 'I_am_proud_to_work_for_Esquire_Deposition_Solutions.', 'Alias'] = 'ECS1'\n",
    "#Data_melt.loc[Data_melt['Item'] == 'I_feel_appreciated_for_the_work_I_do.', 'Alias'] = 'ECS2'\n",
    "\n",
    "Data_melt.loc[Data_melt['Item'] ==\"ECS_Q1\",\"Alias\"]=\"ECS1\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"ECS_Q2\",\"Alias\"]=\"ECS2\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"ECS_Q3\",\"Alias\"]=\"ECS3\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"ECS_Q4\",\"Alias\"]=\"ECS4\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"ECS_Q5\",\"Alias\"]=\"ECS5\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"ECS_Q6\",\"Alias\"]=\"ECS6\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"WEC_Q1\",\"Alias\"]=\"WEC1\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"WEC_Q2\",\"Alias\"]=\"WEC2\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"WEC_Q3\",\"Alias\"]=\"WEC3\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"WEC_Q4\",\"Alias\"]=\"WEC4\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"WEC_Q5\",\"Alias\"]=\"WEC5\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CB_Q1\",\"Alias\"]=\"CMPB1\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CB_Q2\",\"Alias\"]=\"CMPB2\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CB_Q3\",\"Alias\"]=\"CMPB3\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CB_Q4\",\"Alias\"]=\"CMPB4\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"PM_Q1\",\"Alias\"]=\"PMGT1\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"PM_Q2\",\"Alias\"]=\"PMGT2\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"PM_Q3\",\"Alias\"]=\"PMGT3\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"PM_Q4\",\"Alias\"]=\"PMGT4\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"PM_Q5\",\"Alias\"]=\"PMGT5\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"PM_Q6\",\"Alias\"]=\"PMGT6\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"PM_Q7\",\"Alias\"]=\"PMGT7\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CDT_Q1\",\"Alias\"]=\"CDT1\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CDT_Q2\",\"Alias\"]=\"CDT2\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CDT_Q3\",\"Alias\"]=\"CDT3\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CDT_Q4\",\"Alias\"]=\"CDT4\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CDT_Q5\",\"Alias\"]=\"CDT5\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CDT_Q6\",\"Alias\"]=\"CDT6\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"COMM_Q1\",\"Alias\"]=\"COMM1\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"COMM_Q2\",\"Alias\"]=\"COMM2\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"COMM_Q3\",\"Alias\"]=\"COMM3\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"COMM_Q4\",\"Alias\"]=\"COMM4\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"TE_Q1\",\"Alias\"]=\"TE1\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"TE_Q2\",\"Alias\"]=\"TE2\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"TE_Q3\",\"Alias\"]=\"TE3\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"TE_Q4\",\"Alias\"]=\"TE4\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"TE_Q5\",\"Alias\"]=\"TE5\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"JS_Q1\",\"Alias\"]=\"STRESS1\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"JS_Q2\",\"Alias\"]=\"STRESS2\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"JS_Q3\",\"Alias\"]=\"STRESS3\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CI_Q1\",\"Alias\"]=\"COIMG1\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CI_Q2\",\"Alias\"]=\"COIMG2\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CI_Q3\",\"Alias\"]=\"COIMG3\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"CI_Q4\",\"Alias\"]=\"COIMG4\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"STRAT_Q1\",\"Alias\"]=\"STRGY1\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"STRAT_Q2\",\"Alias\"]=\"STRGY2\"\n",
    "Data_melt.loc[Data_melt['Item'] ==\"STRAT_Q3\",\"Alias\"]=\"STRGY3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt.head() #Check your columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt #To see full data\n",
    "\n",
    "#write to .csv\n",
    "#Data_melt.to_csv(\"C:\\\\Users\\\\rstilson\\\\Desktop\\\\Python Programming Course\\\\Data_melt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Alias_Area\n",
    "Here we are going to create an Alias Area to further roll up the data. This allows us to simiply grab all of the ECS items by choosing the ECS Alias Area rather than ECS1, ECS2, etc. \n",
    "\n",
    "We will then write to a .csv to have a look at the data because sometimes that is just easier with a spread sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Alias_Area\n",
    "#From: https://stackoverflow.com/questions/21733893/pandas-dataframe-add-a-field-based-on-multiple-if-statements\n",
    "Data_melt['Alias_Area']='NA'\n",
    "#Data_melt.loc[Data_melt['Area'] == 'Employee_Commitment_and_Satisfaction', 'Alias_Area'] = 'ECS'\n",
    "Data_melt.loc[Data_melt['Area'] == \"Employee_Commitment_and_Satisfaction\",\"Alias_Area\"]=\"ECS\"\n",
    "Data_melt.loc[Data_melt['Area'] == \"Workplace_Environment_and_Culture\",\"Alias_Area\"]=\"WEC\"\n",
    "Data_melt.loc[Data_melt['Area'] == \"Compensation_&_Benefits\",\"Alias_Area\"]=\"CMPB\"\n",
    "Data_melt.loc[Data_melt['Area'] == \"Performance_Management\",\"Alias_Area\"]=\"PMGT\"\n",
    "Data_melt.loc[Data_melt['Area'] == \"Career_Development_and_Training\",\"Alias_Area\"]=\"CDT\"\n",
    "Data_melt.loc[Data_melt['Area'] == \"Communications\",\"Alias_Area\"]=\"COMM\"\n",
    "Data_melt.loc[Data_melt['Area'] == \"Team_Effectiveness\",\"Alias_Area\"]=\"TE\"\n",
    "Data_melt.loc[Data_melt['Area'] == \"Job_Stress\",\"Alias_Area\"]=\"STRESS\"\n",
    "Data_melt.loc[Data_melt['Area'] == \"Company_Image\",\"Alias_Area\"]=\"COIMG\"\n",
    "Data_melt.loc[Data_melt['Area'] == \"Strategy\",\"Alias_Area\"]=\"STRGY\"\n",
    "\n",
    "#write to .csv\n",
    "Data_melt.to_csv(\"C:\\\\Users\\\\Rob\\\\Desktop\\\\Python Programming Course\\\\Data_melt.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt.Area.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding values in Python\n",
    "Below we will recode the values of Strongly Disagree - Strongly Agree into numeric values so we can do mean, SD, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Recoding values\n",
    "\n",
    "Data_melt['response']='NA'\n",
    "Data_melt.loc[Data_melt['value'] == \"Strongly Disagree\",\"response\"]= 1\n",
    "Data_melt.loc[Data_melt['value'] == \"Disagree\",\"response\"]= 2\n",
    "Data_melt.loc[Data_melt['value'] == \"Somewhat Disagree\",\"response\"]= 3\n",
    "Data_melt.loc[Data_melt['value'] == \"Somewhat Agree\",\"response\"]= 4\n",
    "Data_melt.loc[Data_melt['value'] == \"Agree\",\"response\"]= 5\n",
    "Data_melt.loc[Data_melt['value'] == \"Strongly Agree\",\"response\"]= 6\n",
    "\n",
    "\n",
    "Data_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recode into 4 choices instead of 6\n",
    "#From: https://stackoverflow.com/questions/27673231/why-should-i-make-a-copy-of-a-data-frame-in-pandas\n",
    "#Data_melt_4op = pd.DataFrame(Data_melt)#this doesn't decouple Data_melt from Data_melt_4op so Data_melt continues to get modifed\\\n",
    "#(see above)\n",
    "Data_melt_4op = Data_melt.copy()\n",
    "Data_melt_4op['response']='NA'\n",
    "Data_melt_4op.loc[Data_melt_4op['value'] == \"Strongly Disagree\",\"response\"]= \"Unfavorable\"\n",
    "Data_melt_4op.loc[Data_melt_4op['value'] == \"Disagree\",\"response\"]= \"Unfavorable\"\n",
    "Data_melt_4op.loc[Data_melt_4op['value'] == \"Somewhat Disagree\",\"response\"]= \"Moderately Unfavorable\"\n",
    "Data_melt_4op.loc[Data_melt_4op['value'] == \"Somewhat Agree\",\"response\"]= \"Moderately Favorable\"\n",
    "Data_melt_4op.loc[Data_melt_4op['value'] == \"Agree\",\"response\"]= \"Favorable\"\n",
    "Data_melt_4op.loc[Data_melt_4op['value'] == \"Strongly Agree\",\"response\"]= \"Favorable\"\n",
    "\n",
    "#Data_melt.head()\n",
    "#Data_melt_4op.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_melt_4op.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate out OEQ (Text) data\n",
    "\n",
    "We don't have any actual Text data under the Open Ended Question (OEQ) variables, but I wanted to show you how to do that because it is very common to have comments in survey data. \n",
    "\n",
    "One thing to keep in mind with tall data is that when dealing with mixed data types (numeric and string) is that once they are in the same column (value), then the entire column will be switched over to string (text) values. This remains even after you seperate out the string values. We can check the data types with the dtypes function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_melt.describe\n",
    "\n",
    "#From: https://stackoverflow.com/questions/41262370/determining-pandas-column-datatype\n",
    "\n",
    "print(Data_melt.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we won't have to convert it, but we will go through the steps anyway since that is more realistic for data you will typically encounter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing all rows that don't start with OEQ\n",
    "#From: https://stackoverflow.com/questions/41689722/how-to-select-rows-start-with-some-str-in-pandas\n",
    "#Data_melt[Data_melt['Area'].isin(['OEQ'])]\n",
    "#Data_melt_num = Data_melt['Area'][~Data_melt.Area.str.get(0).isin(['OEQ'])] #This isn't working yet\n",
    "#Data_melt_num = Data_melt[(Data_melt.Area.str[0][:, None] == ['OEQ']).any(1)]#This isn't working yet\n",
    "#From: https://chrisalbon.com/python/pandas_dropping_column_and_rows.html\n",
    "#Data_melt_num = Data_melt[Data_melt.Area != 'OEQ']\n",
    "\n",
    "#From:https://stackoverflow.com/questions/13851535/how-to-delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression\n",
    "Data_melt_num = Data_melt.drop(Data_melt[(Data_melt.Area == 'OEQ') | (Data_melt.Area == 'NPS')].index)\n",
    "#Data_melt_num\n",
    "#Data_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_melt\n",
    "Data_melt_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert 'response' to numeric\n",
    "#From: https://stackoverflow.com/questions/39694192/convert-string-column-to-integer\n",
    "Data_melt_num.response = (pd.to_numeric(Data_melt_num.response, errors='coerce'))\n",
    "#Data_melt_num = pd.DataFrame(Data_melt_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Data_melt.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_melt_num.response()\n",
    "Data_melt_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing (Groupby) data in Python\n",
    "Since we created our Area earlier, we can now group by that variable to take the mean. We will then sort it within the same line. We also show you how to round to 2 decimals to keep the data neat and readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping into 'Area' and taking the mean\n",
    "#From: https://stackoverflow.com/questions/30482071/how-to-calculate-mean-values-grouped-on-another-column-in-pandas\n",
    "Topic_Area_Means = Data_melt_num.groupby('Area', as_index=False)['response'].mean().sort_values('response') #Sort, From: https://stackoverflow.com/questions/44885933/how-to-sort-bars-in-a-bar-plot-in-ascending-order\n",
    "Topic_Area_Means.response = np.round(Topic_Area_Means.response,2) #From: https://stackoverflow.com/questions/19100540/rounding-entries-in-a-pandas-dafaframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Topic_Area_Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data (Bar Chart) in Python\n",
    "Now we will create a bar chart using another package called matplotlib. This is sort of similar to the ggplot2 package from R, but not nearly as intuitive. We will arrange the data from highest mean to lowest mean and then tack on the mean value and make it bold and blue with a slight adjustment to get the values a little further away from the end of the bar. \n",
    "\n",
    "Data visualization in Python is a huge area that is worth exploring, but is outside of the scope of this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Horizontal Bar chart\n",
    "#From: https://pythonspot.com/en/matplotlib-bar-chart/\n",
    "#Counter()\n",
    "\n",
    "y_pos = np.arange(len(Topic_Area_Means.response))\n",
    "#y_pos = np.arange(len(Topic_Area_Means.Area))\n",
    "plt.barh(y_pos, Topic_Area_Means.response)\n",
    "plt.yticks(y_pos, Topic_Area_Means.Area)\n",
    "\n",
    "#Add in numbers to ends of bars\n",
    "#From: https://stackoverflow.com/questions/30228069/how-to-display-the-value-of-the-bar-on-each-bar-with-pyplot-barh\n",
    "\n",
    "for i, v in enumerate(Topic_Area_Means.response):\n",
    "    plt.text(v + 0.25, i, str(v), color='blue', fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
